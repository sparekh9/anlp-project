{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ecee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import heapq\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import einops\n",
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.visualization import image_grid, visualization_preprocess\n",
    "from prs_hook import hook_prs_logger\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789e8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a raw feature vector and outputs:\n",
    "      - fi (feature vector)\n",
    "      - pi (scalar gate) using sigmoid\n",
    "    Paper Appendix A.1: MulT outputs df+1 dims: first df = fi, last 1 = pi.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, df=64, dropout=.1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, df + 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        fi = out[..., :-1]                # feature vector\n",
    "        #fi = self.dropout(fi)\n",
    "        pi = torch.sigmoid(out[..., -1])  # scalar activation\n",
    "        return fi, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6ab692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossModalBlock(nn.Module):\n",
    "    def __init__(self, dim=768, hidden=1024, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear projections before attention\n",
    "        self.img_proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "        self.txt_proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.attn_i2t = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.attn_t2i = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ln_i_attn = nn.LayerNorm(dim)\n",
    "        self.ln_t_attn = nn.LayerNorm(dim)\n",
    "\n",
    "        # Feed-forward blocks\n",
    "        self.ff_img = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, dim),\n",
    "        )\n",
    "\n",
    "        self.ff_txt = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, dim),\n",
    "        )\n",
    "\n",
    "        self.ln_i_ff = nn.LayerNorm(dim)\n",
    "        self.ln_t_ff = nn.LayerNorm(dim)\n",
    "\n",
    "\n",
    "    def forward(self, img, txt):\n",
    "        \"\"\"\n",
    "        img, txt: [B, 1, dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # ---- Projection + LN ----\n",
    "        img = self.img_proj(img)\n",
    "        txt = self.txt_proj(txt)\n",
    "\n",
    "        # ---- Image-to-Text (I â†’ T) ----\n",
    "        attn_txt, _ = self.attn_i2t(query=txt, key=img, value=img)\n",
    "        txt = self.ln_t_attn(txt + attn_txt)\n",
    "\n",
    "        # ---- Text-to-Image (T â†’ I) ----\n",
    "        attn_img, _ = self.attn_t2i(query=img, key=txt, value=txt)\n",
    "        img = self.ln_i_attn(img + attn_img)\n",
    "\n",
    "        # ---- Feed-forward ----\n",
    "        img2 = self.ff_img(img)\n",
    "        txt2 = self.ff_txt(txt)\n",
    "\n",
    "        img = self.ln_i_ff(img + img2)\n",
    "        txt = self.ln_t_ff(txt + txt2)\n",
    "\n",
    "        return img, txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5331b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionMLP(nn.Module):\n",
    "    def __init__(self, dim=768, fusion_dim=1536, out_dim=768):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt):\n",
    "        \"\"\"\n",
    "        img, txt: [B, 1, dim]\n",
    "        \"\"\"\n",
    "        fused = torch.cat([img[:, 0], txt[:, 0]], dim=-1)  # [B, 2*dim]\n",
    "        return self.fc(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3f8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, dim=768, heads=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block = CrossModalBlock(dim, hidden, heads, dropout)\n",
    "        self.fusion = FusionMLP(dim=dim, fusion_dim=dim*2, out_dim=dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, img_emb, txt_emb):\n",
    "        img = img_emb.unsqueeze(1)  # [B,1,dim]\n",
    "        txt = txt_emb.unsqueeze(1)\n",
    "\n",
    "        img_out, txt_out = self.block(img, txt)\n",
    "        fused = self.fusion(img_out, txt_out)\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c54fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RoutingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Faithful implementation of Multimodal Routing\n",
    "    (Tsai et al., EMNLP 2020).\n",
    "    \n",
    "    Implements:\n",
    "      - Equation (1): routing adjustment\n",
    "      - Equation (2): concept update\n",
    "      - Equation (3): final logits (feature-by-feature)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: int, dc: int, num_concepts: int,\n",
    "                 num_features: int, iters: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.dc = dc\n",
    "        self.num_concepts = num_concepts\n",
    "        self.num_features = num_features\n",
    "        self.iters = iters\n",
    "\n",
    "        # W_ij  â€” projection matrices\n",
    "        # Shape: [num_features, num_concepts, df, dc]\n",
    "        self.W = nn.Parameter(\n",
    "            torch.randn(num_features, num_concepts, df, dc) * 0.3\n",
    "        )\n",
    "\n",
    "        # o_j  â€” output weight vectors (Equation 3)\n",
    "        # Shape: [num_concepts, dc]\n",
    "        self.O = nn.Parameter(\n",
    "            torch.randn(num_concepts, dc) * 0.3\n",
    "        )\n",
    "\n",
    "        # bias term for logits (not explicitly shown in the paper, but included\n",
    "        # because â€œlinear transformationâ€ means affine transform)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_concepts))\n",
    "\n",
    "        # Proper initialization of concepts:\n",
    "        # The paper: â€œConcepts are initialized with uniform weights.â€\n",
    "        # We choose small uniform values for stability.\n",
    "        self.register_buffer(\"C_init\", torch.zeros(num_concepts, dc))\n",
    "\n",
    "    def forward(self, F_list, P_list):\n",
    "        \"\"\"\n",
    "        F_list: list of modality features f_i,       each [batch, df]\n",
    "        P_list: list of modality activations p_i,    each [batch]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch, num_concepts]\n",
    "            R: routing coefficients r_ij\n",
    "            C: final concepts c_j\n",
    "        \"\"\"\n",
    "\n",
    "        batch = F_list[0].size(0)\n",
    "        device = F_list[0].device\n",
    "\n",
    "        # Initialize concepts uniformly\n",
    "        C = self.C_init.unsqueeze(0).expand(batch, -1, -1).clone()\n",
    "\n",
    "        # -------------------------------\n",
    "        #  Routing iterations\n",
    "        # -------------------------------\n",
    "\n",
    "        for _ in range(self.iters):\n",
    "\n",
    "            # ------- Routing adjustment (Eq. 1) -------\n",
    "            S_list = []\n",
    "\n",
    "            for i, fi in enumerate(F_list):\n",
    "                # fi: [batch, df]\n",
    "                # W_i: [num_concepts, df, dc]\n",
    "                W_i = self.W[i]\n",
    "\n",
    "                # proj_ij = f_i W_ij\n",
    "                # proj: [batch, num_concepts, dc]\n",
    "                proj = torch.einsum(\"bd,jdc->bjc\", fi, W_i)\n",
    "\n",
    "                # similarity score with concepts: dot(proj_ij, c_j)\n",
    "                s_ij = (proj * C).sum(dim=-1)  # [batch, num_concepts]\n",
    "                S_list.append(s_ij)\n",
    "\n",
    "            # S: [batch, num_features, num_concepts]\n",
    "            S = torch.stack(S_list, dim=1)\n",
    "\n",
    "            # r_ij: softmax over concepts j (per feature i)\n",
    "            R = F.softmax(S, dim=-1)\n",
    "\n",
    "            # ------- Concept Update (Eq. 2) -------\n",
    "            new_C = torch.zeros_like(C)\n",
    "\n",
    "            for i, fi in enumerate(F_list):\n",
    "                W_i = self.W[i]  # [num_concepts, df, dc]\n",
    "                proj = torch.einsum(\"bd,jdc->bjc\", fi, W_i)  # [batch, num_concepts, dc]\n",
    "\n",
    "                pi  = P_list[i]        # [batch]\n",
    "                rij = R[:, i, :]       # [batch, num_concepts]\n",
    "\n",
    "                # weight per sample + concept\n",
    "                weight = pi.unsqueeze(-1) * rij  # [batch, num_concepts]\n",
    "                new_C += weight.unsqueeze(-1) * proj\n",
    "\n",
    "            C = new_C\n",
    "\n",
    "        # -------------------------------\n",
    "        #  Prediction Stage (Eq. 3)\n",
    "        # -------------------------------\n",
    "        # logit_j = Î£_i p_i r_ij * (o_jáµ€ (f_i W_ij))   + bias_j\n",
    "\n",
    "        logits = torch.zeros(batch, self.num_concepts, device=device)\n",
    "\n",
    "        for i, fi in enumerate(F_list):\n",
    "            W_i = self.W[i]\n",
    "            proj = torch.einsum(\"bd,jdc->bjc\", fi, W_i)  # [batch, num_concepts, dc]\n",
    "\n",
    "            # o_jáµ€ (f_i W_ij)\n",
    "            contrib = (proj * self.O.unsqueeze(0)).sum(dim=-1)  # [batch, num_concepts]\n",
    "\n",
    "            pi  = P_list[i]\n",
    "            rij = R[:, i, :]\n",
    "\n",
    "            # sum contributions across all features\n",
    "            logits += pi.unsqueeze(-1) * rij * contrib\n",
    "\n",
    "        logits = logits + self.bias.unsqueeze(0)\n",
    "\n",
    "        return logits, R, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688fb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPMultimodalRoutingHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal Routing head for:\n",
    "      - image feature\n",
    "      - image+text feature\n",
    "      - text feature\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_image: int,       # e.g. 1024 from CLIP visual encoder\n",
    "        dim_image_text: int,  # your joint feature dim\n",
    "        dim_text: int,        # e.g. 768 from CLIP text encoder\n",
    "        num_classes: int,     # number of labels / concepts\n",
    "        df: int = 256,        # feature dimension\n",
    "        dc: int = 256,        # concept dimension\n",
    "        routing_iters: int = 2,\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        device = 'cuda:0'\n",
    "        pretrained = 'laion2b_s32b_b82k' # 'laion2b_s32b_b79k'\n",
    "        model_name = 'ViT-L-14' # 'ViT-H-14'\n",
    "        self.clip, _, _ = create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "        \n",
    "        self.feature_names = [\"image\", \"image_text\", \"text\"]\n",
    "        self.num_features = len(self.feature_names)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Project each feature to (fi, pi)\n",
    "        self.proj_image      = FeatureProjector(dim_image, df, dropout)\n",
    "        self.proj_image_text = FeatureProjector(dim_image_text, df, dropout)\n",
    "        self.proj_text       = FeatureProjector(dim_text, df, dropout)\n",
    "\n",
    "        self.routing = RoutingLayer(\n",
    "            df=df,\n",
    "            dc=dc,\n",
    "            num_concepts=num_classes,\n",
    "            num_features=self.num_features,\n",
    "            iters=routing_iters\n",
    "        )\n",
    "        self.fusion_text_image = CrossModalFusion(dim=dim_image_text, dropout=dropout, heads=16)\n",
    "        self.pi_image = nn.Linear(dim_image, 1)\n",
    "        self.pi_joint = nn.Linear(dim_image_text, 1)\n",
    "        self.pi_text = nn.Linear(dim_text, 1)\n",
    "\n",
    "\n",
    "    def forward(self, f_image, f_text):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          f_image:      [batch, dim_image]\n",
    "          f_image_text: [batch, dim_image_text]\n",
    "          f_text:       [batch, dim_text]\n",
    "\n",
    "        Returns:\n",
    "          logits: [batch, num_classes]\n",
    "          routing_info: dict with:\n",
    "            - R: [batch, num_features, num_classes]\n",
    "            - C: [batch, num_classes, dc]\n",
    "            - P: [batch, num_features] pi activations\n",
    "        \"\"\"\n",
    "\n",
    "        f_image  = self.clip.encode_image(f_image)      # [B, 768]\n",
    "        f_text  = self.clip.encode_text(f_text) # [B, 768]\n",
    "        f_joint = self.fusion_text_image(f_image, f_text)\n",
    "        # Encode each feature to (fi, pi)\n",
    "        fi_img,  _ = self.proj_image(f_image)\n",
    "        fi_joint,_ = self.proj_image_text(f_joint)\n",
    "        fi_txt,  _ = self.proj_text(f_text)\n",
    "\n",
    "        # 3) Gating logits Pi(x)\n",
    "        pi_img  = self.pi_image(f_image)    # [B, 1]\n",
    "        pi_joint= self.pi_joint(f_joint)    # [B, 1]\n",
    "        pi_txt  = self.pi_text(f_text)      # [B, 1]\n",
    "\n",
    "        pi_stack = torch.cat([pi_img, pi_joint, pi_txt], dim=1)\n",
    "        pi_soft  = torch.softmax(pi_stack, dim=1)\n",
    "\n",
    "        F_list = [fi_img, fi_joint, fi_txt]\n",
    "        # Use the normalized softmax weights\n",
    "        pi_img_norm   = pi_soft[:, 0]   # [B]\n",
    "        pi_joint_norm = pi_soft[:, 1]   # [B]\n",
    "        pi_txt_norm   = pi_soft[:, 2]   # [B]\n",
    "\n",
    "        P_list = [pi_img_norm, pi_joint_norm, pi_txt_norm]\n",
    "\n",
    "\n",
    "        logits, R, C = self.routing(F_list, P_list)\n",
    "\n",
    "\n",
    "        # Stack pi for convenience: [batch, num_features]\n",
    "        P = torch.stack(P_list, dim=1)\n",
    "\n",
    "        routing_info = {\n",
    "            \"R\": R,  # routing coeffs rij\n",
    "            \"C\": C,  # concepts\n",
    "            \"P\": P,  # pi activations\n",
    "            \"feature_order\": self.feature_names\n",
    "        }\n",
    "        return logits, routing_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c0ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = 'cuda:0'\n",
    "pretrained = 'laion2b_s32b_b82k' # 'laion2b_s32b_b79k'\n",
    "model_name = 'ViT-L-14' # 'ViT-H-14'\n",
    "\n",
    "class TwoImageDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, image_only_root, image_text_root, clip_model=\"ViT-L/14\", device=\"cpu\"):\n",
    "        self.items = [json.loads(line) for line in open(jsonl_path)]\n",
    "        self.image_only = image_only_root\n",
    "        self.image_text= image_text_root\n",
    "\n",
    "        # CLIP preprocessing\n",
    "        self.clip_model, self.preprocess, _ = create_model_and_transforms(clip_model, pretrained=pretrained)\n",
    "        self.tokenizer = get_tokenizer(clip_model)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.items[idx]\n",
    "\n",
    "        name = entry['img'].split('/')[-1]\n",
    "        img_path1 = os.path.join(self.image_only, name)\n",
    "        img_path2 = os.path.join(self.image_text, name)\n",
    "\n",
    "        img1 = Image.open(img_path1).convert(\"RGB\")\n",
    "        img2 = Image.open(img_path2).convert(\"RGB\")\n",
    "\n",
    "        img1 = self.preprocess(img1)   # [3,224,224]\n",
    "        img2 = self.preprocess(img2)   # [3,224,224]\n",
    "\n",
    "        # --- Tokenize text ---\n",
    "        text_tokens = self.tokenizer([entry['text']]).squeeze()\n",
    "\n",
    "        # --- Label ---\n",
    "        label = torch.tensor(entry[\"label\"], dtype=torch.long)\n",
    "\n",
    "        return img1.to(self.device), img2.to(self.device), text_tokens.to(self.device), label.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fef59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = 'cuda:0'\n",
    "pretrained = 'laion2b_s32b_b82k'\n",
    "model_name = 'ViT-L-14'\n",
    "\n",
    "class TwoImageDatasetPreload(Dataset):\n",
    "    def __init__(self, jsonl_path, image_only_root, image_text_root,\n",
    "                 clip_model=\"ViT-L/14\", device=\"cpu\"):\n",
    "\n",
    "        # Load json lines\n",
    "        self.items = [json.loads(line) for line in open(jsonl_path)]\n",
    "        self.image_only = image_only_root\n",
    "        self.image_text = image_text_root\n",
    "        self.device = device\n",
    "\n",
    "        # CLIP preprocessors\n",
    "        self.clip_model, _, self.preprocess = create_model_and_transforms(\n",
    "            clip_model, pretrained=pretrained\n",
    "        )\n",
    "        self.tokenizer = get_tokenizer(clip_model)\n",
    "\n",
    "        # In-memory storage\n",
    "        self.images1 = []\n",
    "        self.images2 = []\n",
    "        self.text_tokens = []\n",
    "        self.labels = []\n",
    "\n",
    "        print(\"ðŸ”„ Preloading dataset into memory...\")\n",
    "\n",
    "        for entry in self.items:\n",
    "            name = entry['img'].split('/')[-1]\n",
    "\n",
    "            # --- Load both images now ---\n",
    "            img_path1 = os.path.join(self.image_only, name)\n",
    "            img_path2 = os.path.join(self.image_text, name)\n",
    "\n",
    "            img1 = Image.open(img_path1).convert(\"RGB\")\n",
    "            img2 = Image.open(img_path2).convert(\"RGB\")\n",
    "\n",
    "            img1 = self.preprocess(img1)   # Tensor\n",
    "            img2 = self.preprocess(img2)\n",
    "\n",
    "            # --- Tokenize text once ---\n",
    "            txt = self.tokenizer([entry['text']]).squeeze()\n",
    "\n",
    "            # --- Label ---\n",
    "            lab = torch.tensor(entry['label'], dtype=torch.long)\n",
    "\n",
    "            # Move to memory (optionally GPU)\n",
    "            self.images1.append(img1.to(self.device))\n",
    "            self.images2.append(img2.to(self.device))\n",
    "            self.text_tokens.append(txt.to(self.device))\n",
    "            self.labels.append(lab.to(self.device))\n",
    "\n",
    "        print(f\"âœ… Finished loading {len(self.items)} samples into memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.images1[idx],\n",
    "            self.images2[idx],\n",
    "            self.text_tokens[idx],\n",
    "            self.labels[idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "512456b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport easyocr\\nimport cv2\\nimport numpy as np\\nfrom tqdm import tqdm\\n\\nreader = easyocr.Reader([\\'en\\'])  # <-- Load once (massive speedup)\\n\\nfor entry in tqdm(range(len(data))):\\n    img_path = f\"data/{data[entry][\\'img\\']}\"\\n    filename = img_path.split(\\'/\\')[-1]\\n    img = cv2.imread(img_path)\\n\\n    results = reader.readtext(img_path)\\n\\n    mask = np.zeros(img.shape[:2], dtype=np.uint8)\\n\\n    for (bbox, text, prob) in results:\\n        pts = np.array(bbox, dtype=np.int32)\\n        cv2.fillPoly(mask, [pts], 255)\\n\\n    output = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA)\\n\\n    cv2.imwrite(f\"data/img_clean/{filename}\", output)\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "reader = easyocr.Reader(['en'])  # <-- Load once (massive speedup)\n",
    "\n",
    "for entry in tqdm(range(len(data))):\n",
    "    img_path = f\"data/{data[entry]['img']}\"\n",
    "    filename = img_path.split('/')[-1]\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    results = reader.readtext(img_path)\n",
    "\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    for (bbox, text, prob) in results:\n",
    "        pts = np.array(bbox, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [pts], 255)\n",
    "\n",
    "    output = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA)\n",
    "\n",
    "    cv2.imwrite(f\"data/img_clean/{filename}\", output)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d1e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_two_images(batch):\n",
    "    img1 = torch.stack([b[0] for b in batch], dim=0)     # [B, 3, 224, 224]\n",
    "    img2 = torch.stack([b[1] for b in batch], dim=0)     # [B, 3, 224, 224]\n",
    "    texts = torch.stack([b[2] for b in batch], dim=0)    # [B, 77]\n",
    "    labels = torch.stack([b[3] for b in batch], dim=0)   # [B]\n",
    "    return img1, img2, texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaa56ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = TwoImageDataset(\n",
    "    clip_model=model_name,\n",
    "    jsonl_path=\"data/train.jsonl\",\n",
    "    image_only_root=\"./data/img_clean\",\n",
    "    image_text_root=\"./data/img\",\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_two_images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce57eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Preloading dataset into memory...\n",
      "âœ… Finished loading 500 samples into memory.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TwoImageDatasetPreload(\n",
    "    clip_model=model_name,\n",
    "    jsonl_path=\"data/dev.jsonl\",\n",
    "    image_only_root=\"./data/img_clean\",\n",
    "    image_text_root=\"./data/img\",\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_two_images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf91919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99a706d68a24382ba091a0d0d292f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23594756e52d4f508768064c2c25a605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5020\n",
      "Test F1 Score: 0.0604\n",
      "Test AUC: 0.5991\n",
      "Test Loss: 0.9395834969149696\n",
      "Train Loss: 0.6848319394880509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46a29cea3454476acd06eb35bdc340e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5660\n",
      "Test F1 Score: 0.4182\n",
      "Test AUC: 0.6116\n",
      "Test Loss: 0.7837992116572365\n",
      "Train Loss: 0.6560717254936814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc02c40ced33471c8223c8f24c6d53f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5540\n",
      "Test F1 Score: 0.2921\n",
      "Test AUC: 0.6137\n",
      "Test Loss: 0.7537310279551006\n",
      "Train Loss: 0.6527770085630812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7583be5d70ea435d9b2000208f93210d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5760\n",
      "Test F1 Score: 0.4646\n",
      "Test AUC: 0.6178\n",
      "Test Loss: 0.6997273398770226\n",
      "Train Loss: 0.6312519371308185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0413538aee7f40c7a9102ac76cc0b49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5100\n",
      "Test F1 Score: 0.0684\n",
      "Test AUC: 0.6129\n",
      "Test Loss: 0.9828819831212362\n",
      "Train Loss: 0.6247581161582489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b239885fd8f4eaea9f6a2dba9a19a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5480\n",
      "Test F1 Score: 0.3687\n",
      "Test AUC: 0.6054\n",
      "Test Loss: 0.6963636941379971\n",
      "Train Loss: 0.62035190510384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    101\u001b[39m optimizer.step()\n\u001b[32m    102\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m running_loss.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i%\u001b[32m250\u001b[39m==\u001b[32m249\u001b[39m:\n\u001b[32m    105\u001b[39m     acc, f1, auc, test_loss = test(head, test_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Example dims (adjust to your actual setup)\n",
    "head = CLIPMultimodalRoutingHead(\n",
    "    dim_image=768,\n",
    "    dim_image_text=768,  # e.g. concat(img_emb, text_emb)\n",
    "    dim_text=768,\n",
    "    num_classes=2,   # e.g. binary classification (hateful vs not)\n",
    "    df=64,\n",
    "    dc=64,\n",
    "    routing_iters=2,\n",
    "    dropout = .25\n",
    ").to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    head.parameters(),\n",
    "    lr=1e-5,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "def build_features(images_i, images_it, tokenized_text):\n",
    "    # 1. Get CLIP embeddings\n",
    "    with torch.no_grad():\n",
    "        img_emb  = clip.encode_image(images_i)              # or clip.encode_image(images)\n",
    "        txt_emb  = clip.encode_text(tokenized_text) # text encoder\n",
    "        img_t_emb = torch.cat([img_emb, txt_emb], dim=-1)\n",
    "\n",
    "    # 2. Your image-text joint feature â€” example: concatenation\n",
    "    return img_emb, img_t_emb, txt_emb\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print('test')\n",
    "\n",
    "def test(head, test_loader):\n",
    "    head.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    all_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images_i, images_it, tokenized_text, labels in tqdm(test_loader):\n",
    "            # ----- Extract CLIP embeddings -----\n",
    "            images_i = images_i.to(device)\n",
    "            tokenized_text = tokenized_text.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Joint multimodal feature (matches your head config)\n",
    "\n",
    "            # ----- Run through routing head -----\n",
    "            logits, routing_info = head(images_i, tokenized_text)\n",
    "\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            all_losses.append(loss.item())\n",
    "            # Softmax probabilities (for AUC)\n",
    "            probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "            preds = (probs >= 0.5).long()\n",
    "\n",
    "            # Collect results\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_probs.extend(probs.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # ----- Compute metrics -----\n",
    "    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "\n",
    "    return acc, f1, auc, sum(all_losses)/len(all_losses)\n",
    "\n",
    "avg_loss = 0\n",
    "running_loss = []\n",
    "accs = []\n",
    "f1s = []\n",
    "aucs = []\n",
    "test_losses = []\n",
    "head.train()\n",
    "\n",
    "for i, (images_i, images_it, tokenized_text, labels) in tqdm(enumerate(loader)):\n",
    "    logits, routing_info = head(images_i, tokenized_text)\n",
    "    probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "    preds = (probs >= 0.5).long()    \n",
    " \n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    running_loss.append(loss.item())\n",
    "    if i%250==249:\n",
    "        acc, f1, auc, test_loss = test(head, test_loader)\n",
    "        accs.append(acc)\n",
    "        f1s.append(f1)\n",
    "        aucs.append(auc)\n",
    "        test_losses.append(test_loss)\n",
    "        head.train()\n",
    "        print(f\"Test Loss: {test_losses[-1]}\")\n",
    "        print(f\"Train Loss: {sum(running_loss)/i}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bcdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
